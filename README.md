# bert-base-model-tuning

#### Overview
The BERT model  is a neural network-based, often pre-trained, word embedding model. The model takes words as inputs and embed them into ```R^d``` space. BERT distinguishes itself from other embedding model by being bidirectionally contextual, i.e. when BERT embeds a word, BERT takes into account words that come before and after it. This method of embedding allows for better results in prediction tasks such as machine translation and automated question-answering."

Implemented on Google Colab.
